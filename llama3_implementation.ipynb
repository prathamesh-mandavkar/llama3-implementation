{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math"
      ],
      "metadata": {
        "id": "JOHHIHcjeWzN"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "v = 10\n",
        "seq_len = 5\n",
        "b = 1\n",
        "tokens = torch.randint(v, (b, seq_len))\n",
        "tokens.shape, tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEje5kPPeW1Y",
        "outputId": "ef198755-1b67-4db0-9d38-ea1b58f7a9f2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5]), tensor([[1, 7, 1, 9, 0]]))"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d = 16\n",
        "embedding = nn.Embedding(v, d)\n",
        "embedding.weight.shape, embedding.weight"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BItItwY2eW3m",
        "outputId": "4b80bfb4-30f7-4d1e-adfa-f5e70c71d99a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10, 16]),\n",
              " Parameter containing:\n",
              " tensor([[ 9.1225e-02,  2.0366e+00,  1.9383e-01, -1.9696e+00,  1.1776e+00,\n",
              "          -5.1408e-01,  3.2720e-01,  5.9374e-02,  5.5193e-01, -7.6948e-01,\n",
              "          -5.3258e-01, -8.3575e-02,  6.4998e-01, -5.8064e-01,  6.1310e-01,\n",
              "           8.6126e-01],\n",
              "         [ 4.8720e-01, -8.7291e-01,  1.2219e+00, -1.1459e+00, -2.1013e+00,\n",
              "           3.9205e-01, -3.1630e-01, -2.9296e-01, -1.8151e+00, -1.3193e+00,\n",
              "           3.1812e-01, -2.6833e-01, -1.4210e+00,  9.5136e-01, -9.5099e-01,\n",
              "          -9.1863e-01],\n",
              "         [-4.7831e-01, -2.3978e+00,  2.5013e-02,  6.6511e-01,  1.4395e+00,\n",
              "          -4.7337e-01, -1.4695e+00,  8.1029e-02,  9.8985e-01,  5.7672e-01,\n",
              "          -1.3192e+00,  1.1745e+00, -2.2438e+00, -5.2124e-01,  3.0874e-01,\n",
              "           1.7272e-01],\n",
              "         [ 1.3685e-01,  9.5971e-01, -4.0739e-01,  3.1784e+00,  9.9384e-02,\n",
              "           5.2807e-02,  3.8992e-02,  1.8404e+00, -1.0096e+00, -4.5549e-01,\n",
              "          -6.2814e-01, -3.0709e-02,  4.1311e-01, -5.5749e-01, -6.1639e-01,\n",
              "          -1.4128e-01],\n",
              "         [-3.3814e-01, -1.0725e+00,  1.9764e+00, -1.8498e+00, -2.5789e+00,\n",
              "           4.1528e-02, -4.2074e-01,  5.0523e-01,  1.4625e-01,  7.0470e-01,\n",
              "           3.3931e-01, -1.0260e-03, -3.0599e-01,  2.7035e-04,  7.8436e-01,\n",
              "          -1.2578e+00],\n",
              "         [ 6.8243e-01,  1.2152e+00,  2.4017e-01,  3.3159e-01, -2.2417e+00,\n",
              "          -7.6272e-01,  6.2837e-01,  2.7247e-02, -1.6215e+00,  6.5465e-01,\n",
              "          -5.4199e-01, -9.5958e-01, -9.5597e-01,  8.7792e-01, -2.3372e-01,\n",
              "           1.1641e+00],\n",
              "         [ 1.8014e+00, -2.0026e-01,  3.4119e-01,  1.4476e+00,  4.9265e-01,\n",
              "           5.9859e-01, -3.1621e-01,  1.7036e+00, -1.5866e+00,  7.2370e-01,\n",
              "          -6.1524e-01, -7.8867e-01,  9.0834e-02, -4.2232e-01,  1.7766e-01,\n",
              "           3.5053e-01],\n",
              "         [ 1.8185e+00, -1.4255e-01, -3.4887e-01, -2.1035e-01, -6.6782e-01,\n",
              "          -9.6761e-01,  2.7134e-01, -1.5767e+00,  1.6402e+00, -1.8149e+00,\n",
              "          -9.5165e-01,  1.2237e+00, -8.8386e-01,  5.7446e-01, -5.4614e-01,\n",
              "           5.7105e-01],\n",
              "         [-5.1415e-01,  7.2778e-01, -1.1667e+00, -5.1504e-01,  3.4585e-01,\n",
              "           1.1143e-01,  3.7442e-01, -1.3680e-01,  1.3406e+00, -1.8343e+00,\n",
              "          -1.2112e+00, -3.0389e-01,  5.3115e-01, -4.5153e-01,  4.4744e-01,\n",
              "          -2.2870e+00],\n",
              "         [ 2.0418e+00, -3.7065e-01, -4.1632e-01,  1.2082e+00,  1.2690e-01,\n",
              "           1.1172e+00,  2.1350e-01,  1.0372e+00,  6.8986e-01,  2.7725e+00,\n",
              "           6.1101e-01,  1.0755e+00, -1.0651e+00,  3.8275e-01, -3.1296e-01,\n",
              "          -1.2047e+00]], requires_grad=True))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = embedding(tokens)\n",
        "x.shape, x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LqKNqorqSshT",
        "outputId": "6b9d1e9e-c681-4d45-803c-6b3a6155ca72"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 16]),\n",
              " tensor([[[ 0.4872, -0.8729,  1.2219, -1.1459, -2.1013,  0.3920, -0.3163,\n",
              "           -0.2930, -1.8151, -1.3193,  0.3181, -0.2683, -1.4210,  0.9514,\n",
              "           -0.9510, -0.9186],\n",
              "          [ 1.8185, -0.1426, -0.3489, -0.2103, -0.6678, -0.9676,  0.2713,\n",
              "           -1.5767,  1.6402, -1.8149, -0.9517,  1.2237, -0.8839,  0.5745,\n",
              "           -0.5461,  0.5711],\n",
              "          [ 0.4872, -0.8729,  1.2219, -1.1459, -2.1013,  0.3920, -0.3163,\n",
              "           -0.2930, -1.8151, -1.3193,  0.3181, -0.2683, -1.4210,  0.9514,\n",
              "           -0.9510, -0.9186],\n",
              "          [ 2.0418, -0.3707, -0.4163,  1.2082,  0.1269,  1.1172,  0.2135,\n",
              "            1.0372,  0.6899,  2.7725,  0.6110,  1.0755, -1.0651,  0.3827,\n",
              "           -0.3130, -1.2047],\n",
              "          [ 0.0912,  2.0366,  0.1938, -1.9696,  1.1776, -0.5141,  0.3272,\n",
              "            0.0594,  0.5519, -0.7695, -0.5326, -0.0836,  0.6500, -0.5806,\n",
              "            0.6131,  0.8613]]], grad_fn=<EmbeddingBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "theta = 10000\n",
        "num_heads = 4\n",
        "head_dim = d // num_heads\n",
        "\n",
        "freqs = 1.0 / (theta ** (torch.arange(0, head_dim, 2)[: (head_dim // 2)].float() / head_dim))\n",
        "print(f'freqs: {freqs.shape}\\n{freqs}\\n')\n",
        "\n",
        "t = torch.arange(seq_len * 2, device=freqs.device, dtype=torch.float32)\n",
        "print(f't: {t.shape}\\n{t}\\n')\n",
        "\n",
        "freqs = torch.outer(t, freqs)\n",
        "print(f'freqs: {freqs.shape}\\n{freqs}\\n')\n",
        "\n",
        "freqs_cis = torch.polar(torch.ones_like(freqs), freqs)[:seq_len]\n",
        "print(f'freqs_cis: {freqs_cis.shape}\\n{freqs_cis}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Y0jy0fjeAhs",
        "outputId": "b8f6e07c-c3aa-4ccc-928d-8239fe3f9898"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "freqs: torch.Size([2])\n",
            "tensor([1.0000, 0.0100])\n",
            "\n",
            "t: torch.Size([10])\n",
            "tensor([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.])\n",
            "\n",
            "freqs: torch.Size([10, 2])\n",
            "tensor([[0.0000, 0.0000],\n",
            "        [1.0000, 0.0100],\n",
            "        [2.0000, 0.0200],\n",
            "        [3.0000, 0.0300],\n",
            "        [4.0000, 0.0400],\n",
            "        [5.0000, 0.0500],\n",
            "        [6.0000, 0.0600],\n",
            "        [7.0000, 0.0700],\n",
            "        [8.0000, 0.0800],\n",
            "        [9.0000, 0.0900]])\n",
            "\n",
            "freqs_cis: torch.Size([5, 2])\n",
            "tensor([[ 1.0000+0.0000j,  1.0000+0.0000j],\n",
            "        [ 0.5403+0.8415j,  0.9999+0.0100j],\n",
            "        [-0.4161+0.9093j,  0.9998+0.0200j],\n",
            "        [-0.9900+0.1411j,  0.9996+0.0300j],\n",
            "        [-0.6536-0.7568j,  0.9992+0.0400j]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.full(\n",
        "    (seq_len, seq_len),\n",
        "    float(\"-inf\")\n",
        ")\n",
        "mask = torch.triu(mask, diagonal=1)\n",
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eL1OwBVjxiC",
        "outputId": "df09e6d2-3a16-42cb-f7a7-7437bccab96e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., -inf, -inf, -inf, -inf],\n",
              "        [0., 0., -inf, -inf, -inf],\n",
              "        [0., 0., 0., -inf, -inf],\n",
              "        [0., 0., 0., 0., -inf],\n",
              "        [0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h = x\n",
        "print(f'h: {h.shape}\\n{h}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz7KOCA3oYBY",
        "outputId": "1f33acbb-5967-4941-d78a-9118354c6274"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h: torch.Size([1, 5, 16])\n",
            "tensor([[[ 0.4872, -0.8729,  1.2219, -1.1459, -2.1013,  0.3920, -0.3163,\n",
            "          -0.2930, -1.8151, -1.3193,  0.3181, -0.2683, -1.4210,  0.9514,\n",
            "          -0.9510, -0.9186],\n",
            "         [ 1.8185, -0.1426, -0.3489, -0.2103, -0.6678, -0.9676,  0.2713,\n",
            "          -1.5767,  1.6402, -1.8149, -0.9517,  1.2237, -0.8839,  0.5745,\n",
            "          -0.5461,  0.5711],\n",
            "         [ 0.4872, -0.8729,  1.2219, -1.1459, -2.1013,  0.3920, -0.3163,\n",
            "          -0.2930, -1.8151, -1.3193,  0.3181, -0.2683, -1.4210,  0.9514,\n",
            "          -0.9510, -0.9186],\n",
            "         [ 2.0418, -0.3707, -0.4163,  1.2082,  0.1269,  1.1172,  0.2135,\n",
            "           1.0372,  0.6899,  2.7725,  0.6110,  1.0755, -1.0651,  0.3827,\n",
            "          -0.3130, -1.2047],\n",
            "         [ 0.0912,  2.0366,  0.1938, -1.9696,  1.1776, -0.5141,  0.3272,\n",
            "           0.0594,  0.5519, -0.7695, -0.5326, -0.0836,  0.6500, -0.5806,\n",
            "           0.6131,  0.8613]]], grad_fn=<EmbeddingBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_squared = x.pow(2).mean(dim=-1, keepdim=True)\n",
        "mean_squared"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M-Oz0Gw2Z7qd",
        "outputId": "4aa88090-a788-413a-be7f-3aa42d733b0c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1.1526],\n",
              "         [1.0974],\n",
              "         [1.1526],\n",
              "         [1.3030],\n",
              "         [0.8062]]], grad_fn=<MeanBackward1>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_normed = x * torch.rsqrt(mean_squared + 1e-6)\n",
        "print(f'x_normed: {x_normed.shape}\\n{x_normed}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IE0hSKbSa06u",
        "outputId": "cb02c6b1-eb8c-45dd-e0b8-6120ccaf90db"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "x_normed: torch.Size([1, 5, 16])\n",
            "tensor([[[ 0.4538, -0.8131,  1.1381, -1.0674, -1.9573,  0.3652, -0.2946,\n",
            "          -0.2729, -1.6907, -1.2288,  0.2963, -0.2499, -1.3236,  0.8862,\n",
            "          -0.8858, -0.8557],\n",
            "         [ 1.7359, -0.1361, -0.3330, -0.2008, -0.6375, -0.9237,  0.2590,\n",
            "          -1.5051,  1.5658, -1.7325, -0.9085,  1.1682, -0.8437,  0.5484,\n",
            "          -0.5213,  0.5451],\n",
            "         [ 0.4538, -0.8131,  1.1381, -1.0674, -1.9573,  0.3652, -0.2946,\n",
            "          -0.2729, -1.6907, -1.2288,  0.2963, -0.2499, -1.3236,  0.8862,\n",
            "          -0.8858, -0.8557],\n",
            "         [ 1.7888, -0.3247, -0.3647,  1.0584,  0.1112,  0.9787,  0.1870,\n",
            "           0.9086,  0.6044,  2.4289,  0.5353,  0.9422, -0.9330,  0.3353,\n",
            "          -0.2742, -1.0553],\n",
            "         [ 0.1016,  2.2682,  0.2159, -2.1936,  1.3116, -0.5725,  0.3644,\n",
            "           0.0661,  0.6147, -0.8570, -0.5932, -0.0931,  0.7239, -0.6467,\n",
            "           0.6828,  0.9592]]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rms_scale = torch.ones(d)\n",
        "print(f'rms_scale: {rms_scale.shape}\\n{rms_scale}\\n')\n",
        "\n",
        "x_normed *= rms_scale\n",
        "print(f'x_normed: {x_normed.shape}\\n{x_normed}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_Z4LQQnbuNG",
        "outputId": "d8bac29b-25e5-47d6-cd34-22d5b1cbe8e2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rms_scale: torch.Size([16])\n",
            "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
            "\n",
            "x_normed: torch.Size([1, 5, 16])\n",
            "tensor([[[ 0.4538, -0.8131,  1.1381, -1.0674, -1.9573,  0.3652, -0.2946,\n",
            "          -0.2729, -1.6907, -1.2288,  0.2963, -0.2499, -1.3236,  0.8862,\n",
            "          -0.8858, -0.8557],\n",
            "         [ 1.7359, -0.1361, -0.3330, -0.2008, -0.6375, -0.9237,  0.2590,\n",
            "          -1.5051,  1.5658, -1.7325, -0.9085,  1.1682, -0.8437,  0.5484,\n",
            "          -0.5213,  0.5451],\n",
            "         [ 0.4538, -0.8131,  1.1381, -1.0674, -1.9573,  0.3652, -0.2946,\n",
            "          -0.2729, -1.6907, -1.2288,  0.2963, -0.2499, -1.3236,  0.8862,\n",
            "          -0.8858, -0.8557],\n",
            "         [ 1.7888, -0.3247, -0.3647,  1.0584,  0.1112,  0.9787,  0.1870,\n",
            "           0.9086,  0.6044,  2.4289,  0.5353,  0.9422, -0.9330,  0.3353,\n",
            "          -0.2742, -1.0553],\n",
            "         [ 0.1016,  2.2682,  0.2159, -2.1936,  1.3116, -0.5725,  0.3644,\n",
            "           0.0661,  0.6147, -0.8570, -0.5932, -0.0931,  0.7239, -0.6467,\n",
            "           0.6828,  0.9592]]], grad_fn=<MulBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(torch.nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self._norm(x.float()).type_as(x)\n",
        "        return output * self.weight"
      ],
      "metadata": {
        "id": "L7biEvzBSsnr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h, x_normed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpQZkXhLc9RJ",
        "outputId": "3751bc70-8420-438f-8035-1805f03f3701"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 0.4872, -0.8729,  1.2219, -1.1459, -2.1013,  0.3920, -0.3163,\n",
              "           -0.2930, -1.8151, -1.3193,  0.3181, -0.2683, -1.4210,  0.9514,\n",
              "           -0.9510, -0.9186],\n",
              "          [ 1.8185, -0.1426, -0.3489, -0.2103, -0.6678, -0.9676,  0.2713,\n",
              "           -1.5767,  1.6402, -1.8149, -0.9517,  1.2237, -0.8839,  0.5745,\n",
              "           -0.5461,  0.5711],\n",
              "          [ 0.4872, -0.8729,  1.2219, -1.1459, -2.1013,  0.3920, -0.3163,\n",
              "           -0.2930, -1.8151, -1.3193,  0.3181, -0.2683, -1.4210,  0.9514,\n",
              "           -0.9510, -0.9186],\n",
              "          [ 2.0418, -0.3707, -0.4163,  1.2082,  0.1269,  1.1172,  0.2135,\n",
              "            1.0372,  0.6899,  2.7725,  0.6110,  1.0755, -1.0651,  0.3827,\n",
              "           -0.3130, -1.2047],\n",
              "          [ 0.0912,  2.0366,  0.1938, -1.9696,  1.1776, -0.5141,  0.3272,\n",
              "            0.0594,  0.5519, -0.7695, -0.5326, -0.0836,  0.6500, -0.5806,\n",
              "            0.6131,  0.8613]]], grad_fn=<EmbeddingBackward0>),\n",
              " tensor([[[ 0.4538, -0.8131,  1.1381, -1.0674, -1.9573,  0.3652, -0.2946,\n",
              "           -0.2729, -1.6907, -1.2288,  0.2963, -0.2499, -1.3236,  0.8862,\n",
              "           -0.8858, -0.8557],\n",
              "          [ 1.7359, -0.1361, -0.3330, -0.2008, -0.6375, -0.9237,  0.2590,\n",
              "           -1.5051,  1.5658, -1.7325, -0.9085,  1.1682, -0.8437,  0.5484,\n",
              "           -0.5213,  0.5451],\n",
              "          [ 0.4538, -0.8131,  1.1381, -1.0674, -1.9573,  0.3652, -0.2946,\n",
              "           -0.2729, -1.6907, -1.2288,  0.2963, -0.2499, -1.3236,  0.8862,\n",
              "           -0.8858, -0.8557],\n",
              "          [ 1.7888, -0.3247, -0.3647,  1.0584,  0.1112,  0.9787,  0.1870,\n",
              "            0.9086,  0.6044,  2.4289,  0.5353,  0.9422, -0.9330,  0.3353,\n",
              "           -0.2742, -1.0553],\n",
              "          [ 0.1016,  2.2682,  0.2159, -2.1936,  1.3116, -0.5725,  0.3644,\n",
              "            0.0661,  0.6147, -0.8570, -0.5932, -0.0931,  0.7239, -0.6467,\n",
              "            0.6828,  0.9592]]], grad_fn=<MulBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_kv_heads = 2\n",
        "assert num_heads % num_kv_heads == 0\n",
        "print(f\"as a reminder: num_heads = {num_heads}, head_dim = {head_dim}\")"
      ],
      "metadata": {
        "id": "eDl4ypypdK4C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bb4b702-8447-415c-be7f-21f3d92da623"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "as a reminder: num_heads = 4, head_dim = 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wq = nn.Linear(d, num_heads * head_dim, bias=False)\n",
        "wk = nn.Linear(d, num_kv_heads * head_dim, bias=False)\n",
        "wv = nn.Linear(d, num_kv_heads * head_dim, bias=False)\n",
        "print(\"Attention weights: \", wq.weight.shape, wk.weight.shape, wv.weight.shape)\n",
        "\n",
        "xq = wq(x_normed)\n",
        "xk = wk(x_normed)\n",
        "xv = wv(x_normed)\n",
        "print(\"Attention projections: \", xq.shape, xk.shape, xv.shape)\n",
        "\n",
        "xq = xq.view(b, seq_len, num_heads, head_dim)\n",
        "xk = xk.view(b, seq_len, num_kv_heads, head_dim)\n",
        "xv = xv.view(b, seq_len, num_kv_heads, head_dim)\n",
        "print(\"Reshaped: \", xq.shape, xk.shape, xv.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH0rZAHnffMn",
        "outputId": "fd7331c0-359a-4f74-a6af-61397c5b23e8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attention weights:  torch.Size([16, 16]) torch.Size([8, 16]) torch.Size([8, 16])\n",
            "Attention projections:  torch.Size([1, 5, 16]) torch.Size([1, 5, 8]) torch.Size([1, 5, 8])\n",
            "Reshaped:  torch.Size([1, 5, 4, 4]) torch.Size([1, 5, 2, 4]) torch.Size([1, 5, 2, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xq = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "xk = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "print(f'xq: {xq.shape}\\n{xq}\\n')\n",
        "print(f'xk: {xk.shape}\\n{xk}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sFhg0hOUQyi",
        "outputId": "6fa0b808-e355-42c5-dd6e-84ed30f4f0a8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xq: torch.Size([1, 5, 4, 2])\n",
            "tensor([[[[-0.4291+0.1962j,  0.5342-0.4326j],\n",
            "          [ 0.7279-0.0046j,  0.6127-1.0880j],\n",
            "          [-0.6104+0.3684j,  0.4189+0.2181j],\n",
            "          [-0.3447+0.0437j,  0.0332+1.4734j]],\n",
            "\n",
            "         [[ 0.3149+0.0414j,  0.1118-0.6002j],\n",
            "          [ 0.0803+0.4188j, -0.5217+0.3098j],\n",
            "          [ 0.3095-0.1339j, -0.8670-0.7433j],\n",
            "          [-0.6699+0.1433j, -0.2096-0.9590j]],\n",
            "\n",
            "         [[-0.4291+0.1962j,  0.5342-0.4326j],\n",
            "          [ 0.7279-0.0046j,  0.6127-1.0880j],\n",
            "          [-0.6104+0.3684j,  0.4189+0.2181j],\n",
            "          [-0.3447+0.0437j,  0.0332+1.4734j]],\n",
            "\n",
            "         [[-0.0009+1.2107j, -0.8004+0.2490j],\n",
            "          [ 0.1001+0.2970j,  0.1697-0.0677j],\n",
            "          [ 0.5461+0.8427j, -0.3851-0.2928j],\n",
            "          [-0.1326-0.3128j,  0.6575-0.4605j]],\n",
            "\n",
            "         [[ 0.3308+0.3233j,  0.7262-0.3959j],\n",
            "          [ 0.6365-0.1578j,  0.3134+0.2363j],\n",
            "          [-0.2056-0.9324j,  0.6066+0.4276j],\n",
            "          [-0.4668+0.9308j, -0.0943-0.1814j]]]],\n",
            "       grad_fn=<ViewAsComplexBackward0>)\n",
            "\n",
            "xk: torch.Size([1, 5, 2, 2])\n",
            "tensor([[[[-0.1916-0.9420j,  0.4892+0.9625j],\n",
            "          [ 0.6461+0.1081j, -0.5578+0.4774j]],\n",
            "\n",
            "         [[ 0.9480+0.7053j,  0.8005-0.2444j],\n",
            "          [-0.2498+0.5932j,  0.2144-0.2818j]],\n",
            "\n",
            "         [[-0.1916-0.9420j,  0.4892+0.9625j],\n",
            "          [ 0.6461+0.1081j, -0.5578+0.4774j]],\n",
            "\n",
            "         [[-0.5423+0.5909j, -0.2537-0.2776j],\n",
            "          [ 0.5084-0.1906j, -0.1282-0.1376j]],\n",
            "\n",
            "         [[ 0.3889+0.6889j, -0.1583-0.1830j],\n",
            "          [ 0.4332+0.1148j,  0.2884-0.6351j]]]],\n",
            "       grad_fn=<ViewAsComplexBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ndim = xq.ndim\n",
        "assert 0 <= 1 < ndim\n",
        "assert freqs_cis.shape == (xq.shape[1], xq.shape[-1]), f'freqs_cis.shape {freqs_cis.shape} != xq.shape[1], xq.shape[-1] {(xq.shape[1], xq.shape[-1])}'\n",
        "\n",
        "shape = [d if i == 1 or i == xq.ndim - 1 else 1 for i, d in enumerate(xq.shape)]\n",
        "print(f'shape: {shape}\\n')\n",
        "\n",
        "freqs_cis = freqs_cis.view(*shape)\n",
        "print(f'freqs_cis: {freqs_cis.shape}\\n{freqs_cis}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62knt5SXUQ3T",
        "outputId": "0b21cf59-0c36-4e00-f169-d30fd067971a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: [1, 5, 1, 2]\n",
            "\n",
            "freqs_cis: torch.Size([1, 5, 1, 2])\n",
            "tensor([[[[ 1.0000+0.0000j,  1.0000+0.0000j]],\n",
            "\n",
            "         [[ 0.5403+0.8415j,  0.9999+0.0100j]],\n",
            "\n",
            "         [[-0.4161+0.9093j,  0.9998+0.0200j]],\n",
            "\n",
            "         [[-0.9900+0.1411j,  0.9996+0.0300j]],\n",
            "\n",
            "         [[-0.6536-0.7568j,  0.9992+0.0400j]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xq = torch.view_as_real(xq * freqs_cis).flatten(3).type_as(xv)\n",
        "xk = torch.view_as_real(xk * freqs_cis).flatten(3).type_as(xv)\n",
        "print(f'xq: {xq.shape}\\n{xq}\\n')\n",
        "print(f'xk: {xk.shape}\\n{xk}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8RyblxHYURBT",
        "outputId": "8779c8a7-9308-4179-a012-2e08b483b696"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "xq: torch.Size([1, 5, 4, 4])\n",
            "tensor([[[[-4.2913e-01,  1.9618e-01,  5.3422e-01, -4.3260e-01],\n",
            "          [ 7.2795e-01, -4.5819e-03,  6.1274e-01, -1.0880e+00],\n",
            "          [-6.1038e-01,  3.6835e-01,  4.1887e-01,  2.1810e-01],\n",
            "          [-3.4472e-01,  4.3680e-02,  3.3236e-02,  1.4734e+00]],\n",
            "\n",
            "         [[ 1.3534e-01,  2.8732e-01,  1.1781e-01, -5.9904e-01],\n",
            "          [-3.0900e-01,  2.9386e-01, -5.2476e-01,  3.0456e-01],\n",
            "          [ 2.7990e-01,  1.8814e-01, -8.5950e-01, -7.5190e-01],\n",
            "          [-4.8248e-01, -4.8626e-01, -2.0000e-01, -9.6103e-01]],\n",
            "\n",
            "         [[ 1.9200e-04, -4.7184e-01,  5.4276e-01, -4.2183e-01],\n",
            "          [-2.9877e-01,  6.6383e-01,  6.3438e-01, -1.0755e+00],\n",
            "          [-8.0936e-02, -7.0831e-01,  4.1443e-01,  2.2643e-01],\n",
            "          [ 1.0373e-01, -3.3163e-01,  3.7640e-03,  1.4737e+00]],\n",
            "\n",
            "         [[-1.6993e-01, -1.1987e+00, -8.0754e-01,  2.2493e-01],\n",
            "          [-1.4101e-01, -2.7986e-01,  1.7163e-01, -6.2605e-02],\n",
            "          [-6.5951e-01, -7.5718e-01, -3.7610e-01, -3.0421e-01],\n",
            "          [ 1.7538e-01,  2.9097e-01,  6.7105e-01, -4.4056e-01]],\n",
            "\n",
            "         [[ 2.8464e-02, -4.6172e-01,  7.4146e-01, -3.6658e-01],\n",
            "          [-5.3543e-01, -3.7858e-01,  3.0369e-01,  2.4865e-01],\n",
            "          [-5.7128e-01,  7.6505e-01,  5.8906e-01,  4.5153e-01],\n",
            "          [ 1.0096e+00, -2.5514e-01, -8.6991e-02, -1.8501e-01]]]],\n",
            "       grad_fn=<ViewBackward0>)\n",
            "\n",
            "xk: torch.Size([1, 5, 2, 4])\n",
            "tensor([[[[-0.1916, -0.9420,  0.4892,  0.9625],\n",
            "          [ 0.6461,  0.1081, -0.5578,  0.4774]],\n",
            "\n",
            "         [[-0.0812,  1.1788,  0.8029, -0.2363],\n",
            "          [-0.6341,  0.1103,  0.2173, -0.2796]],\n",
            "\n",
            "         [[ 0.9363,  0.2178,  0.4698,  0.9721],\n",
            "          [-0.3672,  0.5425, -0.5672,  0.4662]],\n",
            "\n",
            "         [[ 0.4535, -0.6615, -0.2453, -0.2850],\n",
            "          [-0.4764,  0.2605, -0.1240, -0.1414]],\n",
            "\n",
            "         [[ 0.2672, -0.7446, -0.1509, -0.1892],\n",
            "          [-0.1963, -0.4028,  0.3136, -0.6230]]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if num_kv_heads != num_heads:\n",
        "  num_queries_per_kv = num_heads // num_kv_heads\n",
        "  xk = torch.repeat_interleave(xk, num_queries_per_kv, dim=2)\n",
        "  xv = torch.repeat_interleave(xv, num_queries_per_kv, dim=2)\n",
        "\n",
        "xq.shape, xk.shape, xv.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aVeOQpzrs1Ej",
        "outputId": "dcba574a-d2d2-4a6d-e20d-05d31db618dc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 4, 4]), torch.Size([1, 5, 4, 4]), torch.Size([1, 5, 4, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "xq = xq.transpose(1, 2)\n",
        "xk = xk.transpose(1, 2)\n",
        "xv = xv.transpose(1, 2)\n",
        "\n",
        "xq.shape, xk.shape, xv.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmOLwaqDs1MS",
        "outputId": "58b67a03-d884-4bef-8ddf-f6d4988999d3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 4, 5, 4]), torch.Size([1, 4, 5, 4]), torch.Size([1, 4, 5, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = torch.matmul(xq, xk.transpose(2, 3))\n",
        "\n",
        "scores = scores / math.sqrt(head_dim)\n",
        "\n",
        "scores.shape, scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-CCcntgts1fG",
        "outputId": "22ba56d2-b18a-48cd-b6fd-9aa8e6faf301"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 4, 5, 5]),\n",
              " tensor([[[[-1.2882e-01,  3.9863e-01, -2.6431e-01, -1.6605e-01, -1.2974e-01],\n",
              "           [-4.0777e-01,  2.8193e-01, -1.6883e-01,  6.5865e-03, -4.1105e-02],\n",
              "           [ 1.5198e-01, -1.0384e-02, -1.2882e-01,  1.4967e-01,  1.7464e-01],\n",
              "           [ 4.9163e-01, -1.0504e+00, -2.9046e-01,  4.2492e-01,  4.6320e-01],\n",
              "           [ 2.1969e-01,  6.7667e-02, -4.0950e-02,  1.2049e-01,  1.5443e-01]],\n",
              " \n",
              "          [[-4.4131e-01,  3.4228e-01, -4.4563e-02,  2.4650e-01,  1.5564e-01],\n",
              "           [-9.0590e-02, -6.0891e-02, -8.7912e-02, -1.4631e-01, -1.3989e-01],\n",
              "           [-6.4649e-01,  7.8515e-01, -4.4131e-01, -2.1182e-01, -2.3316e-01],\n",
              "           [ 1.5718e-01, -8.2928e-02, -8.6603e-02,  4.8466e-02,  7.8325e-02],\n",
              "           [ 4.2356e-01, -1.0886e-01, -9.9706e-02, -6.8874e-02,  2.2981e-02]],\n",
              " \n",
              "          [[-2.4204e-01,  2.2885e-01,  1.4402e-01,  1.5196e-01, -1.6548e-02],\n",
              "           [ 1.6082e-01, -6.6600e-02,  6.8155e-02,  6.4296e-02,  3.4095e-02],\n",
              "           [-1.2597e-01, -5.7485e-05, -2.4204e-01, -1.1467e-01,  1.4505e-01],\n",
              "           [-2.2172e-01,  1.6900e-01, -4.8551e-02,  1.0331e-01,  2.5303e-01],\n",
              "           [-1.9970e-01,  2.2419e-01,  2.5059e-01,  1.6725e-01, -1.4632e-01]],\n",
              " \n",
              "          [[ 2.3344e-01, -9.0687e-02,  4.0913e-01, -1.8450e-02, -4.2873e-01],\n",
              "           [-3.5579e-01,  2.3878e-01, -2.1060e-01,  1.3195e-01,  4.1331e-01],\n",
              "           [ 3.6634e-01, -2.5682e-01,  2.3344e-01, -1.7234e-01, -4.0189e-01],\n",
              "           [-2.1993e-01,  9.4939e-02, -2.4628e-01, -1.4336e-02,  1.6665e-01],\n",
              "           [ 2.9246e-01, -3.1774e-01, -2.7302e-01, -2.5522e-01, -3.7038e-03]]]],\n",
              "        grad_fn=<DivBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = scores + mask\n",
        "\n",
        "scores.shape, scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPmJK1zbwjuS",
        "outputId": "0aea8717-4fec-4ccf-f56c-a23a77d7ed78"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 4, 5, 5]),\n",
              " tensor([[[[-1.2882e-01,        -inf,        -inf,        -inf,        -inf],\n",
              "           [-4.0777e-01,  2.8193e-01,        -inf,        -inf,        -inf],\n",
              "           [ 1.5198e-01, -1.0384e-02, -1.2882e-01,        -inf,        -inf],\n",
              "           [ 4.9163e-01, -1.0504e+00, -2.9046e-01,  4.2492e-01,        -inf],\n",
              "           [ 2.1969e-01,  6.7667e-02, -4.0950e-02,  1.2049e-01,  1.5443e-01]],\n",
              " \n",
              "          [[-4.4131e-01,        -inf,        -inf,        -inf,        -inf],\n",
              "           [-9.0590e-02, -6.0891e-02,        -inf,        -inf,        -inf],\n",
              "           [-6.4649e-01,  7.8515e-01, -4.4131e-01,        -inf,        -inf],\n",
              "           [ 1.5718e-01, -8.2928e-02, -8.6603e-02,  4.8466e-02,        -inf],\n",
              "           [ 4.2356e-01, -1.0886e-01, -9.9706e-02, -6.8874e-02,  2.2981e-02]],\n",
              " \n",
              "          [[-2.4204e-01,        -inf,        -inf,        -inf,        -inf],\n",
              "           [ 1.6082e-01, -6.6600e-02,        -inf,        -inf,        -inf],\n",
              "           [-1.2597e-01, -5.7485e-05, -2.4204e-01,        -inf,        -inf],\n",
              "           [-2.2172e-01,  1.6900e-01, -4.8551e-02,  1.0331e-01,        -inf],\n",
              "           [-1.9970e-01,  2.2419e-01,  2.5059e-01,  1.6725e-01, -1.4632e-01]],\n",
              " \n",
              "          [[ 2.3344e-01,        -inf,        -inf,        -inf,        -inf],\n",
              "           [-3.5579e-01,  2.3878e-01,        -inf,        -inf,        -inf],\n",
              "           [ 3.6634e-01, -2.5682e-01,  2.3344e-01,        -inf,        -inf],\n",
              "           [-2.1993e-01,  9.4939e-02, -2.4628e-01, -1.4336e-02,        -inf],\n",
              "           [ 2.9246e-01, -3.1774e-01, -2.7302e-01, -2.5522e-01, -3.7038e-03]]]],\n",
              "        grad_fn=<AddBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
        "scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Knsd9x-iwjxd",
        "outputId": "a31b901b-63c3-4c65-c297-df67b8e5dd9a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.3341, 0.6659, 0.0000, 0.0000, 0.0000],\n",
              "          [0.3838, 0.3263, 0.2899, 0.0000, 0.0000],\n",
              "          [0.3836, 0.0821, 0.1755, 0.3588, 0.0000],\n",
              "          [0.2236, 0.1921, 0.1723, 0.2025, 0.2095]],\n",
              "\n",
              "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.4926, 0.5074, 0.0000, 0.0000, 0.0000],\n",
              "          [0.1559, 0.6526, 0.1914, 0.0000, 0.0000],\n",
              "          [0.2884, 0.2269, 0.2260, 0.2587, 0.0000],\n",
              "          [0.2889, 0.1697, 0.1712, 0.1766, 0.1936]],\n",
              "\n",
              "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.5566, 0.4434, 0.0000, 0.0000, 0.0000],\n",
              "          [0.3306, 0.3750, 0.2944, 0.0000, 0.0000],\n",
              "          [0.1980, 0.2926, 0.2354, 0.2740, 0.0000],\n",
              "          [0.1516, 0.2317, 0.2379, 0.2189, 0.1599]],\n",
              "\n",
              "         [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "          [0.3556, 0.6444, 0.0000, 0.0000, 0.0000],\n",
              "          [0.4146, 0.2223, 0.3630, 0.0000, 0.0000],\n",
              "          [0.2187, 0.2996, 0.2130, 0.2686, 0.0000],\n",
              "          [0.2913, 0.1582, 0.1655, 0.1684, 0.2166]]]],\n",
              "       grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = torch.matmul(scores, xv)\n",
        "output.shape, output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtEbx0D1wj0K",
        "outputId": "bd28dd98-192d-4753-d74c-967948e14361"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 4, 5, 4]),\n",
              " tensor([[[[ 0.6480, -0.5029, -1.0608, -0.2744],\n",
              "           [-0.3112, -0.0214, -0.3650, -0.3829],\n",
              "           [ 0.1780, -0.2669, -0.7198, -0.3276],\n",
              "           [ 0.4929, -0.1775, -0.1519,  0.0971],\n",
              "           [-0.0167, -0.0672, -0.2497, -0.1531]],\n",
              " \n",
              "          [[ 0.6480, -0.5029, -1.0608, -0.2744],\n",
              "           [-0.0829, -0.1360, -0.5306, -0.3571],\n",
              "           [-0.2921, -0.0310, -0.3789, -0.3808],\n",
              "           [ 0.2946, -0.1471, -0.2303, -0.0339],\n",
              "           [ 0.0462, -0.1138, -0.3436, -0.1723]],\n",
              " \n",
              "          [[-0.2318, -0.3154, -0.1987, -0.1365],\n",
              "           [-0.0119,  0.1223, -0.0089, -0.1421],\n",
              "           [-0.0458,  0.0548, -0.0382, -0.1412],\n",
              "           [-0.1699,  0.0096, -0.0344, -0.2951],\n",
              "           [ 0.0055,  0.0258, -0.0597, -0.3398]],\n",
              " \n",
              "          [[-0.2318, -0.3154, -0.1987, -0.1365],\n",
              "           [ 0.0878,  0.3207,  0.0772, -0.1446],\n",
              "           [-0.1215, -0.0959, -0.1035, -0.1393],\n",
              "           [-0.1648,  0.0158, -0.0321, -0.2921],\n",
              "           [ 0.0513, -0.0238, -0.0953, -0.3375]]]],\n",
              "        grad_fn=<UnsafeViewBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = output.transpose(1, 2).contiguous().view(b, seq_len, -1)\n",
        "output.shape, output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bpSgyBA41vV",
        "outputId": "71bd91e2-cc30-4b15-d521-53a3eb0156a4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 16]),\n",
              " tensor([[[ 0.6480, -0.5029, -1.0608, -0.2744,  0.6480, -0.5029, -1.0608,\n",
              "           -0.2744, -0.2318, -0.3154, -0.1987, -0.1365, -0.2318, -0.3154,\n",
              "           -0.1987, -0.1365],\n",
              "          [-0.3112, -0.0214, -0.3650, -0.3829, -0.0829, -0.1360, -0.5306,\n",
              "           -0.3571, -0.0119,  0.1223, -0.0089, -0.1421,  0.0878,  0.3207,\n",
              "            0.0772, -0.1446],\n",
              "          [ 0.1780, -0.2669, -0.7198, -0.3276, -0.2921, -0.0310, -0.3789,\n",
              "           -0.3808, -0.0458,  0.0548, -0.0382, -0.1412, -0.1215, -0.0959,\n",
              "           -0.1035, -0.1393],\n",
              "          [ 0.4929, -0.1775, -0.1519,  0.0971,  0.2946, -0.1471, -0.2303,\n",
              "           -0.0339, -0.1699,  0.0096, -0.0344, -0.2951, -0.1648,  0.0158,\n",
              "           -0.0321, -0.2921],\n",
              "          [-0.0167, -0.0672, -0.2497, -0.1531,  0.0462, -0.1138, -0.3436,\n",
              "           -0.1723,  0.0055,  0.0258, -0.0597, -0.3398,  0.0513, -0.0238,\n",
              "           -0.0953, -0.3375]]], grad_fn=<ViewBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wo = nn.Linear(num_heads * head_dim, d, bias=False)\n",
        "Xout = wo(output)\n",
        "Xout.shape, Xout"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paC0J9h549KI",
        "outputId": "c89013d0-5008-4843-f3d5-fac116cfe712"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 16]),\n",
              " tensor([[[-0.6617, -0.0299, -0.4058,  0.2500,  0.0009,  0.1392, -0.4186,\n",
              "            0.3082,  0.7888,  0.2122, -0.1651, -0.1097, -0.5544, -0.1342,\n",
              "            0.0739,  0.1340],\n",
              "          [ 0.0026,  0.0206,  0.0619, -0.2046,  0.1187, -0.0892, -0.2912,\n",
              "            0.1807,  0.1706,  0.2084,  0.0763, -0.0542, -0.1686,  0.0852,\n",
              "           -0.1680, -0.0985],\n",
              "          [-0.2085,  0.0279, -0.1022, -0.0251,  0.1744,  0.0978, -0.2955,\n",
              "            0.1807,  0.4125,  0.1452, -0.0240,  0.0638, -0.0503,  0.0967,\n",
              "           -0.0959,  0.0883],\n",
              "          [-0.1453, -0.0315, -0.1794,  0.0930,  0.0528,  0.1274, -0.1429,\n",
              "            0.1725,  0.2777, -0.0251, -0.1649, -0.0332, -0.2049, -0.1167,\n",
              "           -0.0285,  0.0384],\n",
              "          [-0.0028, -0.0288, -0.0941, -0.0902,  0.1460,  0.0550, -0.1392,\n",
              "            0.1928,  0.2829,  0.1459,  0.0012, -0.0146, -0.2165, -0.0160,\n",
              "           -0.0584,  0.0478]]], grad_fn=<UnsafeViewBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "h += Xout\n",
        "h.shape, h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdVEPPxi6hcc",
        "outputId": "923d1f9a-cade-463c-d91e-6fe08a0424b0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 16]),\n",
              " tensor([[[-0.1745, -0.9029,  0.8161, -0.8959, -2.1005,  0.5312, -0.7349,\n",
              "            0.0153, -1.0263, -1.1071,  0.1530, -0.3781, -1.9754,  0.8171,\n",
              "           -0.8771, -0.7846],\n",
              "          [ 1.8211, -0.1220, -0.2870, -0.4149, -0.5491, -1.0568, -0.0199,\n",
              "           -1.3961,  1.8108, -1.6065, -0.8753,  1.1695, -1.0525,  0.6597,\n",
              "           -0.7141,  0.4726],\n",
              "          [ 0.2787, -0.8450,  1.1197, -1.1710, -1.9269,  0.4898, -0.6118,\n",
              "           -0.1123, -1.4026, -1.1741,  0.2941, -0.2045, -1.4713,  1.0480,\n",
              "           -1.0469, -0.8303],\n",
              "          [ 1.8966, -0.4021, -0.5957,  1.3011,  0.1797,  1.2446,  0.0706,\n",
              "            1.2096,  0.9675,  2.7474,  0.4461,  1.0423, -1.2699,  0.2661,\n",
              "           -0.3415, -1.1662],\n",
              "          [ 0.0884,  2.0078,  0.0998, -2.0598,  1.3237, -0.4591,  0.1880,\n",
              "            0.2522,  0.8348, -0.6235, -0.5314, -0.0982,  0.4335, -0.5966,\n",
              "            0.5547,  0.9091]]], grad_fn=<AddBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pre_ffwd_norm = RMSNorm(d)\n",
        "h_normed = pre_ffwd_norm(h)"
      ],
      "metadata": {
        "id": "Anfh-ziI6hhk"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 4 * d\n",
        "print(hidden_dim)\n",
        "hidden_dim = int(2 * hidden_dim / 3)\n",
        "print(hidden_dim)\n",
        "multiple_of = 256\n",
        "hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
        "print(hidden_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbRg3C6ovcXr",
        "outputId": "e587954b-ea9f-4523-cab3-3270c2f508c0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "64\n",
            "42\n",
            "256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "up = nn.Linear(d, hidden_dim, bias=False)\n",
        "gate = nn.Linear(d, hidden_dim, bias=False)\n",
        "down = nn.Linear(hidden_dim, d, bias=False)"
      ],
      "metadata": {
        "id": "6vp7-QkPufSn"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "up_proj = up(h_normed)\n",
        "print(up_proj.shape, up_proj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtuRJosB0MNf",
        "outputId": "5608d60b-c8d8-44b6-f544-cdc2452d1118"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 256]) tensor([[[-0.0770,  0.1202, -1.1196,  ..., -0.5964,  0.7209, -0.0904],\n",
            "         [-0.7331,  0.2328, -0.6200,  ..., -0.7544, -0.5187, -0.0947],\n",
            "         [-0.2734,  0.0072, -1.0260,  ..., -0.3971,  0.9436, -0.1348],\n",
            "         [ 0.5879,  0.1090, -0.7035,  ..., -0.4524, -0.1750, -0.0806],\n",
            "         [-1.0225, -0.5696,  0.3746,  ...,  0.7557, -0.6852,  0.9189]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gate_proj = F.silu(gate(h_normed))\n",
        "print(gate_proj.shape, gate_proj)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hPlRqhb0MWN",
        "outputId": "833dec7d-eee7-4d36-8aa9-9fee44a9af97"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 256]) tensor([[[ 0.1571,  0.2550,  0.1297,  ...,  0.3237,  0.0146, -0.0486],\n",
            "         [ 0.3652, -0.2275, -0.1862,  ..., -0.2351, -0.1624, -0.1733],\n",
            "         [ 0.1122,  0.3358,  0.2703,  ...,  0.4038, -0.0508,  0.0257],\n",
            "         [-0.2461, -0.2777,  0.1492,  ...,  0.1976, -0.1919, -0.1336],\n",
            "         [-0.1061, -0.0429, -0.2663,  ..., -0.2517,  0.4077, -0.2083]]],\n",
            "       grad_fn=<SiluBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ffwd_output = down(up_proj * gate_proj)\n",
        "print(ffwd_output.shape, ffwd_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iE2bH6-hz2NV",
        "outputId": "552b981a-4f2b-4049-b955-d818e3ecfda9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 16]) tensor([[[-0.0668,  0.0026, -0.0692, -0.0277,  0.2288,  0.0302,  0.0701,\n",
            "           0.0597, -0.0955, -0.2088, -0.1602, -0.1281, -0.1132, -0.0271,\n",
            "           0.0646, -0.0286],\n",
            "         [ 0.2062,  0.1321, -0.0504, -0.0388, -0.0484, -0.0084, -0.0651,\n",
            "          -0.0323, -0.0965,  0.0505, -0.1811, -0.1295, -0.1527,  0.0595,\n",
            "           0.0565,  0.0684],\n",
            "         [-0.0629, -0.0679, -0.0845, -0.0030,  0.1321,  0.0697,  0.0152,\n",
            "          -0.0110, -0.0517, -0.2454, -0.1528, -0.1100, -0.1216, -0.0202,\n",
            "           0.0851, -0.0105],\n",
            "         [-0.0197, -0.0460, -0.0145,  0.0194, -0.0450,  0.1390,  0.1077,\n",
            "          -0.1112, -0.0831, -0.0597, -0.0115,  0.0719,  0.0805, -0.1357,\n",
            "           0.0141,  0.1476],\n",
            "         [-0.0068, -0.1018,  0.2256,  0.0273,  0.1913,  0.0437,  0.1456,\n",
            "          -0.1769,  0.0084, -0.0047,  0.0938, -0.0595, -0.0561, -0.1378,\n",
            "           0.0841,  0.1004]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = h + ffwd_output\n",
        "print(out.shape, out)"
      ],
      "metadata": {
        "id": "Aal1G2gpmp3s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2dd29ff-0ed1-4ccc-e30b-6456296ac158"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 5, 16]) tensor([[[-0.2413, -0.9002,  0.7469, -0.9236, -1.8717,  0.5614, -0.6648,\n",
            "           0.0750, -1.1218, -1.3160, -0.0072, -0.5061, -2.0886,  0.7900,\n",
            "          -0.8125, -0.8132],\n",
            "         [ 2.0273,  0.0101, -0.3374, -0.4538, -0.5975, -1.0653, -0.0850,\n",
            "          -1.4284,  1.7143, -1.5559, -1.0565,  1.0400, -1.2051,  0.7191,\n",
            "          -0.6577,  0.5410],\n",
            "         [ 0.2158, -0.9129,  1.0352, -1.1740, -1.7948,  0.5595, -0.5966,\n",
            "          -0.1233, -1.4543, -1.4194,  0.1413, -0.3145, -1.5930,  1.0279,\n",
            "          -0.9618, -0.8408],\n",
            "         [ 1.8769, -0.4482, -0.6103,  1.3206,  0.1347,  1.3836,  0.1783,\n",
            "           1.0985,  0.8845,  2.6877,  0.4346,  1.1142, -1.1894,  0.1304,\n",
            "          -0.3275, -1.0187],\n",
            "         [ 0.0816,  1.9060,  0.3254, -2.0325,  1.5149, -0.4154,  0.3336,\n",
            "           0.0754,  0.8432, -0.6283, -0.4377, -0.1577,  0.3773, -0.7344,\n",
            "           0.6388,  1.0094]]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_norm = RMSNorm(d)\n",
        "out_normed = final_norm(out)"
      ],
      "metadata": {
        "id": "Apc0fqBT3bkt"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_output = nn.Linear(d, v, bias=False)\n",
        "logits = final_output(out_normed).float()\n",
        "logits.shape, logits"
      ],
      "metadata": {
        "id": "4Do_Yyppmp6-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5181ccd-9030-493c-9644-7251ce0b9d57"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([1, 5, 10]),\n",
              " tensor([[[-1.0293,  0.8460,  0.4050,  0.4700, -0.3143,  0.7373, -0.3887,\n",
              "            0.4419,  0.4680, -0.2215],\n",
              "          [-0.0776, -0.0209,  0.0788, -0.1343,  0.1066,  0.6125,  0.6652,\n",
              "            0.7374, -0.6187,  0.0756],\n",
              "          [-0.9305,  0.6992,  0.2725,  0.1775, -0.5326,  0.5436, -0.5076,\n",
              "            0.5103,  0.1687, -0.4812],\n",
              "          [ 1.3699, -0.9603,  0.4626,  0.4300, -0.1564,  0.5303, -1.4380,\n",
              "           -0.1278, -0.3727,  0.4631],\n",
              "          [-0.2596, -0.1003,  0.0659, -0.2814,  0.3217, -0.0557,  0.6066,\n",
              "           -0.0484, -0.8193, -0.1996]]], grad_fn=<UnsafeViewBackward0>))"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "probs = F.softmax(logits, dim=-1)\n",
        "probs"
      ],
      "metadata": {
        "id": "IWQu_ua-lbpC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d74cd05-294f-431c-8638-97debf9416d0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0.0270, 0.1760, 0.1132, 0.1208, 0.0552, 0.1579, 0.0512, 0.1175,\n",
              "          0.1206, 0.0605],\n",
              "         [0.0742, 0.0785, 0.0868, 0.0701, 0.0892, 0.1479, 0.1559, 0.1676,\n",
              "          0.0432, 0.0865],\n",
              "         [0.0349, 0.1782, 0.1163, 0.1058, 0.0520, 0.1525, 0.0533, 0.1475,\n",
              "          0.1048, 0.0547],\n",
              "         [0.2938, 0.0286, 0.1186, 0.1148, 0.0639, 0.1269, 0.0177, 0.0657,\n",
              "          0.0514, 0.1186],\n",
              "         [0.0781, 0.0916, 0.1082, 0.0765, 0.1398, 0.0958, 0.1858, 0.0965,\n",
              "          0.0447, 0.0830]]], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "greedy_indices = torch.argmax(probs, dim=-1)\n",
        "greedy_indices"
      ],
      "metadata": {
        "id": "Z-5_nzJXlbsB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ebc86406-6eb8-4575-b4f4-21bad98d2815"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 7, 1, 0, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "target_token_indices = torch.randint(0, v, greedy_indices.shape)\n",
        "print(target_token_indices)\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "loss = loss_fn(logits.view(1,v,seq_len), target_token_indices)\n",
        "print(loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wJxo95p3P1KK",
        "outputId": "cbde3921-01bb-47a2-a161-00c610cf06fc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3, 8, 4, 3, 7]])\n",
            "tensor(2.3506, grad_fn=<NllLoss2DBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/prathamesh-mandavkar/llama3-implementation/main/tiny_shakespeare_tokenizer.py\n",
        "# and the tokenizer model\n",
        "!wget https://raw.githubusercontent.com/prathamesh-mandavkar/llama3-implementation/main/tokenizers/tiny_shakespeare_tokenizer_512.model\n",
        "!mkdir -p tokenizers\n",
        "!mv tiny_shakespeare_tokenizer_512.model tokenizers/\n",
        "from tiny_shakespeare_tokenizer import *\n",
        "tokenizer = get_tokenizer(size = 512)"
      ],
      "metadata": {
        "id": "7QtHv6UY0w-1",
        "outputId": "ca721570-6d97-4b06-ab70-c858f683e81a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-22 19:05:01--  https://raw.githubusercontent.com/prathamesh-mandavkar/llama3-implementation/main/tiny_shakespeare_tokenizer.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2180 (2.1K) [text/plain]\n",
            "Saving to: tiny_shakespeare_tokenizer.py\n",
            "\n",
            "tiny_shakespeare_to 100%[===================>]   2.13K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-22 19:05:01 (42.8 MB/s) - tiny_shakespeare_tokenizer.py saved [2180/2180]\n",
            "\n",
            "--2025-04-22 19:05:01--  https://raw.githubusercontent.com/prathamesh-mandavkar/llama3-implementation/main/tokenizers/tiny_shakespeare_tokenizer_512.model\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4298 (4.2K) [application/octet-stream]\n",
            "Saving to: tiny_shakespeare_tokenizer_512.model\n",
            "\n",
            "tiny_shakespeare_to 100%[===================>]   4.20K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-22 19:05:01 (60.0 MB/s) - tiny_shakespeare_tokenizer_512.model saved [4298/4298]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "from typing import Optional, Tuple\n",
        "import time\n",
        "import os\n",
        "import json"
      ],
      "metadata": {
        "id": "suAJ3161DTpz"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ModelArgs:\n",
        "    dim: int = 128\n",
        "    n_layers: int = 8\n",
        "    n_heads: int = 4\n",
        "    n_kv_heads: Optional[int] = 1\n",
        "    vocab_size: int = tokenizer.vocab_len\n",
        "    multiple_of: int = 256\n",
        "    ffn_dim_multiplier: Optional[float] = None\n",
        "    norm_eps: float = 1e-5\n",
        "    rope_theta: float = 10000\n",
        "    max_batch_size: int = 32\n",
        "    max_seq_len: int = 512\n",
        "    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    dropout_rate: float = 0.1"
      ],
      "metadata": {
        "id": "bCsmICWv5rMw"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(torch.nn.Module):\n",
        "    def __init__(self, dim: int, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "\n",
        "    def _norm(self, x):\n",
        "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self._norm(x.float()).type_as(x)\n",
        "        return output * self.weight"
      ],
      "metadata": {
        "id": "VJJUAg5oBpVU"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
        "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
        "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
        "    freqs = torch.outer(t, freqs)\n",
        "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)\n",
        "    return freqs_cis.to(params.device)\n",
        "\n",
        "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
        "    ndim = x.ndim\n",
        "    assert 0 <= 1 < ndim\n",
        "    assert freqs_cis.shape == (x.shape[1], x.shape[-1]), f'freqs_cis.shape {freqs_cis.shape} != (x.shape[1], x.shape[-1]) {(x.shape[1], x.shape[-1])}'\n",
        "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
        "    return freqs_cis.view(*shape)\n",
        "\n",
        "def apply_rotary_emb(\n",
        "    xq: torch.Tensor,\n",
        "    xk: torch.Tensor,\n",
        "    freqs_cis: torch.Tensor,\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
        "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
        "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
        "    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)\n",
        "    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)\n",
        "    return xq_out.type_as(xq), xk_out.type_as(xk)"
      ],
      "metadata": {
        "id": "9bJh8MHTUp7j"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
        "    \"\"\"torch.repeat_interleave(x, dim=2, repeats=n_rep)\"\"\"\n",
        "    bs, seqlen, n_kv_heads, head_dim = x.shape\n",
        "    if n_rep == 1:\n",
        "        return x\n",
        "    return (\n",
        "        x[:, :, :, None, :]\n",
        "        .expand(bs, seqlen, n_kv_heads, n_rep, head_dim)\n",
        "        .reshape(bs, seqlen, n_kv_heads * n_rep, head_dim)\n",
        "    )\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.n_heads = args.n_heads\n",
        "        self.n_kv_heads = args.n_heads if args.n_kv_heads is None else args.n_kv_heads\n",
        "        self.n_rep = args.n_heads // self.n_kv_heads\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "\n",
        "        self.wq = nn.Linear(args.dim, args.n_heads * self.head_dim, bias=False)\n",
        "        self.wk = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
        "        self.wv = nn.Linear(args.dim, self.n_kv_heads * self.head_dim, bias=False)\n",
        "        self.wo = nn.Linear(args.n_heads * self.head_dim, args.dim, bias=False)\n",
        "\n",
        "        self.cache_k = torch.zeros(\n",
        "            (args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim),\n",
        "            requires_grad = False\n",
        "        ).to(args.device)\n",
        "        self.cache_v = torch.zeros(\n",
        "            (args.max_batch_size, args.max_seq_len, self.n_kv_heads, self.head_dim),\n",
        "            requires_grad = False\n",
        "        ).to(args.device)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        freqs_cis: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor],\n",
        "        start_pos: int = None,\n",
        "    ):\n",
        "        bsz, seqlen, _ = x.shape\n",
        "        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)\n",
        "\n",
        "        xq = xq.view(bsz, seqlen, self.n_heads, self.head_dim)\n",
        "        xk = xk.view(bsz, seqlen, self.n_kv_heads, self.head_dim)\n",
        "        xv = xv.view(bsz, seqlen, self.n_kv_heads, self.head_dim)\n",
        "\n",
        "        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
        "\n",
        "        if start_pos is not None:\n",
        "            self.cache_k = self.cache_k.to(xq)\n",
        "            self.cache_v = self.cache_v.to(xq)\n",
        "\n",
        "            self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk\n",
        "            self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv\n",
        "\n",
        "            keys = self.cache_k[:bsz, : start_pos + seqlen]\n",
        "            values = self.cache_v[:bsz, : start_pos + seqlen]\n",
        "        else:\n",
        "            keys, values = xk, xv\n",
        "\n",
        "        keys = repeat_kv(keys, self.n_rep)\n",
        "        values = repeat_kv(values, self.n_rep)\n",
        "\n",
        "        xq = xq.transpose(1, 2)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)\n",
        "        if mask is not None:\n",
        "            scores = scores + mask\n",
        "        scores = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
        "\n",
        "        output = torch.matmul(scores, values)\n",
        "        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)\n",
        "        return self.wo(output)"
      ],
      "metadata": {
        "id": "meweM31MCD97"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2e. Ffwd\n",
        "<a id='twoe'></a>"
      ],
      "metadata": {
        "id": "HW3uAiv-xQWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        hidden_dim: int,\n",
        "        multiple_of: int,\n",
        "        ffn_dim_multiplier: Optional[float],\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        hidden_dim = int(2 * hidden_dim / 3)\n",
        "        if ffn_dim_multiplier is not None:\n",
        "            hidden_dim = int(ffn_dim_multiplier * hidden_dim)\n",
        "        hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n",
        "\n",
        "        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n",
        "        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.w2(F.silu(self.w1(x)) * self.w3(x))"
      ],
      "metadata": {
        "id": "CEf5HsU4UqBf"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2f. Residual Layers\n",
        "<a id='twof'></a>"
      ],
      "metadata": {
        "id": "xF9wBRbNxUTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, layer_id: int, args: ModelArgs):\n",
        "        super().__init__()\n",
        "        self.n_heads = args.n_heads\n",
        "        self.dim = args.dim\n",
        "        self.head_dim = args.dim // args.n_heads\n",
        "        self.attention = Attention(args)\n",
        "        self.feed_forward = FeedForward(\n",
        "            dim=args.dim,\n",
        "            hidden_dim=4 * args.dim,\n",
        "            multiple_of=args.multiple_of,\n",
        "            ffn_dim_multiplier=args.ffn_dim_multiplier,\n",
        "        )\n",
        "        self.layer_id = layer_id\n",
        "        self.attention_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "        self.ffn_norm = RMSNorm(args.dim, eps=args.norm_eps)\n",
        "        self.dropout_rate = args.dropout_rate\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: torch.Tensor,\n",
        "        freqs_cis: torch.Tensor,\n",
        "        mask: Optional[torch.Tensor],\n",
        "        start_pos: int = None,\n",
        "        training = False,\n",
        "    ):\n",
        "\n",
        "        h = x + F.dropout(self.attention(self.attention_norm(x), freqs_cis, mask, start_pos), p=self.dropout_rate, training=training)\n",
        "        out = h + F.dropout(self.feed_forward(self.ffn_norm(h)), p=self.dropout_rate, training=training)\n",
        "        return out"
      ],
      "metadata": {
        "id": "nvfkGSrnUqFY"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Llama3(nn.Module):\n",
        "    def __init__(self, params: ModelArgs, tokenizer):\n",
        "        super().__init__()\n",
        "        self.params = params\n",
        "        self.vocab_size = params.vocab_size\n",
        "        self.n_layers = params.n_layers\n",
        "        self.max_seq_len = params.max_seq_len\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.tok_embeddings = nn.Embedding(params.vocab_size, params.dim)\n",
        "\n",
        "        self.layers = torch.nn.ModuleList()\n",
        "        for layer_id in range(params.n_layers):\n",
        "            self.layers.append(TransformerBlock(layer_id, params))\n",
        "\n",
        "        self.norm = RMSNorm(params.dim, eps=params.norm_eps)\n",
        "        self.output = nn.Linear(\n",
        "            params.dim,\n",
        "            params.vocab_size,\n",
        "            bias=False)\n",
        "\n",
        "        self.freqs_cis = precompute_freqs_cis(\n",
        "            params.dim // params.n_heads,\n",
        "            params.max_seq_len * 2,\n",
        "            params.rope_theta,)\n",
        "\n",
        "        mask = torch.full((params.max_seq_len, params.max_seq_len),\n",
        "                          float(\"-inf\"),\n",
        "                          device=params.device)\n",
        "        mask = torch.triu(mask, diagonal=1)\n",
        "        self.register_buffer('mask', mask)\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, tokens: torch.Tensor, targets: torch.Tensor):\n",
        "        bsz, seqlen = tokens.shape\n",
        "        assert tokens.shape == targets.shape\n",
        "        assert seqlen == self.max_seq_len\n",
        "\n",
        "        h = self.tok_embeddings(tokens)\n",
        "\n",
        "        freqs_cis = self.freqs_cis.to(h.device)\n",
        "        freqs_cis = self.freqs_cis[:seqlen]\n",
        "\n",
        "        for layer in self.layers:\n",
        "            h = layer(\n",
        "                h,\n",
        "                freqs_cis,\n",
        "                self.mask,\n",
        "                start_pos = None,\n",
        "                training = True\n",
        "            )\n",
        "\n",
        "        h = self.norm(h)\n",
        "        logits = self.output(h).float()\n",
        "\n",
        "        loss = self.criterion(\n",
        "            logits.view(bsz * seqlen, self.vocab_size),\n",
        "            targets.reshape(bsz * seqlen))\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def forward_inference(self,\n",
        "                          tokens: torch.Tensor,\n",
        "                          start_pos: int,\n",
        "                          max_context_window: int,\n",
        "                         ):\n",
        "        _bsz, seqlen = tokens.shape\n",
        "        h = self.tok_embeddings(tokens)\n",
        "        self.freqs_cis = self.freqs_cis.to(h.device)\n",
        "        freqs_cis = self.freqs_cis[start_pos : start_pos + seqlen]\n",
        "\n",
        "        mask = self.mask[:seqlen, :seqlen]\n",
        "\n",
        "        mask = torch.hstack(\n",
        "            [torch.zeros((seqlen, start_pos), device=tokens.device), mask]\n",
        "        ).type_as(h)\n",
        "\n",
        "        for layer in self.layers:\n",
        "            h = layer(\n",
        "                h,\n",
        "                freqs_cis,\n",
        "                mask,\n",
        "                start_pos = start_pos\n",
        "            )\n",
        "        h = self.norm(h)\n",
        "        logits = self.output(h).float()\n",
        "        return logits\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def Sampler(\n",
        "        self,\n",
        "        logits: torch.Tensor,\n",
        "        temperature: float,\n",
        "        top_p: float,\n",
        "        top_k: int,\n",
        "    ) -> torch.Tensor:\n",
        "        logits = logits[:,-1,:]\n",
        "        logits.div_(temperature)\n",
        "        probs = torch.softmax(logits, dim=-1, dtype=torch.float)\n",
        "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
        "        probs_sum = torch.cumsum(probs_sort, dim=-1)\n",
        "        top_ps_mask = (probs_sum - probs_sort) > top_p\n",
        "        probs_sort = torch.where(top_ps_mask, 0, probs_sort)\n",
        "        top_ks_mask = torch.arange(probs_idx.shape[-1], device=probs_idx.device)\n",
        "        top_ks_mask = top_ks_mask.expand(probs_idx.shape[0], -1)\n",
        "        top_ks_mask = top_ks_mask >= top_k\n",
        "        probs_sort = torch.where(top_ks_mask, 0, probs_sort)\n",
        "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
        "        probs = torch.gather(probs_sort, dim=-1, index=torch.argsort(probs_idx, dim=-1))\n",
        "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
        "        return next_token_id\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def generate(\n",
        "        self,\n",
        "        prompt: str,\n",
        "        max_gen_len: int = None,\n",
        "        memory_saver_div: int = 1,\n",
        "        temperature: float = 0.6,\n",
        "        top_p: float = 0.9,\n",
        "        top_k: int = tokenizer.vocab_len,\n",
        "    ) -> str:\n",
        "        assert ((memory_saver_div & (memory_saver_div-1)) == 0) & (memory_saver_div > 0), f'memory_saver_div {memory_saver_div} must be power of 2'\n",
        "        max_context_window = self.max_seq_len // memory_saver_div\n",
        "        if max_context_window < self.max_seq_len:\n",
        "            print(f'maximum attention matrix size will be {max_context_window}x{self.max_seq_len} rather than {self.max_seq_len}x{self.max_seq_len}\\n')\n",
        "        tokens = self.tokenizer.encode(prompt)\n",
        "\n",
        "        if max_gen_len is None:\n",
        "            max_gen_len = self.max_seq_len - len(tokens)\n",
        "        elif max_gen_len + len(tokens) > self.max_seq_len:\n",
        "            print(f'capping max_gen_len at max_seq_len={self.max_seq_len} including input\\n')\n",
        "            max_gen_len = self.max_seq_len - len(tokens)\n",
        "        tokens = torch.tensor(tokens, device=self.params.device)\n",
        "        tokens = tokens.unsqueeze(0) if len(tokens.shape)==1 else tokens # jic we need to add a batch dimension\n",
        "        start_pos = max(tokens.shape[1] - max_context_window, 0)\n",
        "\n",
        "        for i in range(max_gen_len):\n",
        "            logits = self.forward_inference(\n",
        "                tokens[:,-max_context_window:],\n",
        "                start_pos = start_pos,\n",
        "                max_context_window = max_context_window\n",
        "            )\n",
        "            next_token = self.Sampler(\n",
        "                logits = logits,\n",
        "                temperature = temperature,\n",
        "                top_p = top_p,\n",
        "                top_k = top_k\n",
        "            )\n",
        "\n",
        "            tokens = torch.cat((tokens, next_token), dim=1)\n",
        "            if tokens.shape[1] >= max_context_window:\n",
        "                start_pos += 1\n",
        "        output = self.tokenizer.decode(tokens.squeeze(0).tolist())\n",
        "        return output"
      ],
      "metadata": {
        "id": "4gRVdIZVUqHi"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O input.txt https://raw.githubusercontent.com/prathamesh-mandavkar/llama3-implementation/main/input.txt\n",
        "\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(text[:200])\n",
        "chars = sorted(list(set(text)))\n",
        "v = len(chars)\n",
        "print(chars)\n",
        "print(v)\n",
        "\n",
        "\n",
        "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "5oTbAKuix5nt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aaa570d-7fd8-44d1-af90-3874feacd7f0"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-22 19:05:02--  https://raw.githubusercontent.com/prathamesh-mandavkar/llama3-implementation/main/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: input.txt\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-04-22 19:05:02 (29.6 MB/s) - input.txt saved [1115394/1115394]\n",
            "\n",
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you\n",
            "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "params = ModelArgs()\n",
        "print(params)\n",
        "model = Llama3(params, tokenizer).to(params.device)\n",
        "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pdPqc-L_VqU",
        "outputId": "a0afa2ab-a092-401a-83e7-b14e49d6dbcd"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ModelArgs(dim=128, n_layers=8, n_heads=4, n_kv_heads=1, vocab_size=512, multiple_of=256, ffn_dim_multiplier=None, norm_eps=1e-05, rope_theta=10000, max_batch_size=32, max_seq_len=512, device='cuda', dropout_rate=0.1)\n",
            "2033.792 K parameters\n",
            "Llama3(\n",
            "  (tok_embeddings): Embedding(512, 128)\n",
            "  (layers): ModuleList(\n",
            "    (0-7): 8 x TransformerBlock(\n",
            "      (attention): Attention(\n",
            "        (wq): Linear(in_features=128, out_features=128, bias=False)\n",
            "        (wk): Linear(in_features=128, out_features=32, bias=False)\n",
            "        (wv): Linear(in_features=128, out_features=32, bias=False)\n",
            "        (wo): Linear(in_features=128, out_features=128, bias=False)\n",
            "      )\n",
            "      (feed_forward): FeedForward(\n",
            "        (w1): Linear(in_features=128, out_features=512, bias=False)\n",
            "        (w2): Linear(in_features=512, out_features=128, bias=False)\n",
            "        (w3): Linear(in_features=128, out_features=512, bias=False)\n",
            "      )\n",
            "      (attention_norm): RMSNorm()\n",
            "      (ffn_norm): RMSNorm()\n",
            "    )\n",
            "  )\n",
            "  (norm): RMSNorm()\n",
            "  (output): Linear(in_features=128, out_features=512, bias=False)\n",
            "  (criterion): CrossEntropyLoss()\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_batch(split, batch_size):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - params.max_seq_len, (batch_size,))\n",
        "    x = torch.stack([data[i:i+params.max_seq_len] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+params.max_seq_len+1] for i in ix])\n",
        "    x, y = x.to(params.device), y.to(params.device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "F6M2WCXs_Vld"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(model, batch_size, eval_iters = 5):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split, batch_size)\n",
        "            logits, loss = model(X, targets=Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "oaiM9_Od_Vnv"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lr_init = 1e-2\n",
        "weight_decay = 0.02\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr_init, weight_decay=weight_decay)\n",
        "max_iters = 1000\n",
        "eval_interval = 50\n",
        "warmup_iters = 10\n",
        "warmup_factor = 1e-3\n",
        "lr_final = 1e-5\n",
        "\n",
        "def lr_lambda(current_iter):\n",
        "    if current_iter < warmup_iters:\n",
        "        return warmup_factor + (1 - warmup_factor) * current_iter / warmup_iters\n",
        "    else:\n",
        "        decay_iters = max_iters - warmup_iters\n",
        "        cosine_decay = 0.5 * (1 + math.cos(math.pi * (current_iter - warmup_iters) / decay_iters))\n",
        "        return max(cosine_decay, lr_final / lr_init)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)"
      ],
      "metadata": {
        "id": "9KhAPH5g_VsX"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "for iter in range(max_iters):\n",
        "    xb, yb = get_batch('train', params.max_batch_size)\n",
        "    logits, loss = model(xb, targets=yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    scheduler.step()\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        current_time = time.time()\n",
        "        elapsed_time = current_time - start_time\n",
        "        losses = estimate_loss(model, params.max_batch_size)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"step {iter:04d}: lr {current_lr:.6f}, train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
        "\n",
        "# Disable anomaly detection after the training loop\n",
        "#torch.autograd.set_detect_anomaly(False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OECt3NLpBGKc",
        "outputId": "01a4acce-47d8-4369-f93e-b4de956bb943"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0000: lr 0.001009, train loss 6.3484, val loss 6.3537, time elapsed: 1.03 seconds\n",
            "step 0050: lr 0.009958, train loss 6.3550, val loss 6.3538, time elapsed: 11.60 seconds\n",
            "step 0100: lr 0.009793, train loss 6.3520, val loss 6.3527, time elapsed: 22.37 seconds\n",
            "step 0150: lr 0.009508, train loss 6.3531, val loss 6.3533, time elapsed: 33.21 seconds\n",
            "step 0200: lr 0.009109, train loss 6.3545, val loss 6.3479, time elapsed: 44.09 seconds\n",
            "step 0250: lr 0.008608, train loss 6.3530, val loss 6.3566, time elapsed: 55.02 seconds\n",
            "step 0300: lr 0.008015, train loss 6.3558, val loss 6.3513, time elapsed: 66.04 seconds\n",
            "step 0350: lr 0.007347, train loss 6.3567, val loss 6.3524, time elapsed: 77.11 seconds\n",
            "step 0400: lr 0.006620, train loss 6.3551, val loss 6.3543, time elapsed: 88.26 seconds\n",
            "step 0450: lr 0.005853, train loss 6.3548, val loss 6.3526, time elapsed: 99.49 seconds\n",
            "step 0500: lr 0.005063, train loss 6.3538, val loss 6.3530, time elapsed: 110.81 seconds\n",
            "step 0550: lr 0.004273, train loss 6.3545, val loss 6.3537, time elapsed: 122.14 seconds\n",
            "step 0600: lr 0.003500, train loss 6.3491, val loss 6.3521, time elapsed: 133.42 seconds\n",
            "step 0650: lr 0.002765, train loss 6.3557, val loss 6.3580, time elapsed: 144.65 seconds\n",
            "step 0700: lr 0.002087, train loss 6.3516, val loss 6.3532, time elapsed: 155.84 seconds\n",
            "step 0750: lr 0.001481, train loss 6.3527, val loss 6.3514, time elapsed: 167.02 seconds\n",
            "step 0800: lr 0.000964, train loss 6.3523, val loss 6.3533, time elapsed: 178.22 seconds\n",
            "step 0850: lr 0.000549, train loss 6.3539, val loss 6.3507, time elapsed: 189.44 seconds\n",
            "step 0900: lr 0.000245, train loss 6.3576, val loss 6.3509, time elapsed: 200.68 seconds\n",
            "step 0950: lr 0.000060, train loss 6.3539, val loss 6.3525, time elapsed: 211.93 seconds\n",
            "step 0999: lr 0.000010, train loss 6.3542, val loss 6.3538, time elapsed: 222.96 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/prathamesh-mandavkar/llama3-implementation/raw/main/models/Llama3_2024-10-05_23-12-27.pth\n",
        "!wget https://github.com/prathamesh-mandavkar/llama3-implementation/raw/main/models/Llama3_2024-10-05_23-12-27.json\n",
        "name = 'Llama3_2024-10-05_23-12-27'\n",
        "\n",
        "with open(f'{name}.json', 'r', encoding='utf-8') as f:\n",
        "    params_dict = json.load(f)\n",
        "\n",
        "params = ModelArgs(**params_dict)\n",
        "params.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model = Llama3(params, tokenizer).to(params.device)\n",
        "path = f'{name}.pth'\n",
        "model.load_state_dict(torch.load(path))\n",
        "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "B2exYhJGvxDt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b59b4036-5b84-4629-dcc3-ed1a11309b7c"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-22 19:28:52--  https://github.com/prathamesh-mandavkar/llama3-implementation/raw/main/models/Llama3_2024-10-05_23-12-27.pth\n",
            "Resolving github.com (github.com)... 140.82.116.4\n",
            "Connecting to github.com (github.com)|140.82.116.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/prathamesh-mandavkar/llama3-implementation/main/models/Llama3_2024-10-05_23-12-27.pth [following]\n",
            "--2025-04-22 19:28:53--  https://raw.githubusercontent.com/prathamesh-mandavkar/llama3-implementation/main/models/Llama3_2024-10-05_23-12-27.pth\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9213026 (8.8M) [application/octet-stream]\n",
            "Saving to: Llama3_2024-10-05_23-12-27.pth\n",
            "\n",
            "Llama3_2024-10-05_2 100%[===================>]   8.79M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2025-04-22 19:28:53 (115 MB/s) - Llama3_2024-10-05_23-12-27.pth saved [9213026/9213026]\n",
            "\n",
            "--2025-04-22 19:28:53--  https://github.com/prathamesh-mandavkar/llama3-implementation/raw/main/models/Llama3_2024-10-05_23-12-27.json\n",
            "Resolving github.com (github.com)... 140.82.116.3\n",
            "Connecting to github.com (github.com)|140.82.116.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/prathamesh-mandavkar/llama3-implementation/main/models/Llama3_2024-10-05_23-12-27.json [following]\n",
            "--2025-04-22 19:28:53--  https://raw.githubusercontent.com/prathamesh-mandavkar/llama3-implementation/main/models/Llama3_2024-10-05_23-12-27.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 245 [text/plain]\n",
            "Saving to: Llama3_2024-10-05_23-12-27.json\n",
            "\n",
            "Llama3_2024-10-05_2 100%[===================>]     245  --.-KB/s    in 0s      \n",
            "\n",
            "2025-04-22 19:28:53 (16.9 MB/s) - Llama3_2024-10-05_23-12-27.json saved [245/245]\n",
            "\n",
            "2033.792 K parameters\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Llama3(\n",
              "  (tok_embeddings): Embedding(512, 128)\n",
              "  (layers): ModuleList(\n",
              "    (0-7): 8 x TransformerBlock(\n",
              "      (attention): Attention(\n",
              "        (wq): Linear(in_features=128, out_features=128, bias=False)\n",
              "        (wk): Linear(in_features=128, out_features=32, bias=False)\n",
              "        (wv): Linear(in_features=128, out_features=32, bias=False)\n",
              "        (wo): Linear(in_features=128, out_features=128, bias=False)\n",
              "      )\n",
              "      (feed_forward): FeedForward(\n",
              "        (w1): Linear(in_features=128, out_features=512, bias=False)\n",
              "        (w2): Linear(in_features=512, out_features=128, bias=False)\n",
              "        (w3): Linear(in_features=128, out_features=512, bias=False)\n",
              "      )\n",
              "      (attention_norm): RMSNorm()\n",
              "      (ffn_norm): RMSNorm()\n",
              "    )\n",
              "  )\n",
              "  (norm): RMSNorm()\n",
              "  (output): Linear(in_features=128, out_features=512, bias=False)\n",
              "  (criterion): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou R\""
      ],
      "metadata": {
        "id": "rK5bkaFmv1dH"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.generate(input_str))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eOlwC4iNQO87",
        "outputId": "b0e39e50-c50c-48ff-b918-d1f2cfba8ac1"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "JULIET:\n",
            "O Romeo, Romeo! wherefore art thou Romeo?\n",
            "\n",
            "JULIET:\n",
            "I will be king, and I shall be not be too much:\n",
            "I am so many to a temple of me.\n",
            "\n",
            "Nurse:\n",
            "Well, so look to be so, thou art revenged on,\n",
            "The time hath I could been to death.\n",
            "\n",
            "Nurse:\n",
            "My lord, I would have I will be all great kiss,\n",
            "And so I pray thee, and in my true heart.\n",
            "\n",
            "Nurse:\n",
            "The morning calls greet me in my life,\n",
            "I will be spoke with him to the face.\n",
            "\n",
            "Nurse:\n",
            "We shall be so, thou art a bark, woman!\n",
            "\n",
            "JULIET:\n",
            "I am forgot to thee to the corse.\n",
            "\n",
            "JULIET:\n",
            "And thou art good madam; let me be gone?\n",
            "\n",
            "Nurse:\n",
            "I am my true, and go intent; and there is not.\n",
            "\n",
            "JULIET:\n",
            "I would it be a courage of me:\n",
            "Then let them hear me to follow us with a tear.\n",
            "\n",
            "JULIET:\n",
            "What say you do you?\n",
            "\n",
            "JULIET:\n",
            "I will not bid you that I can not talk of.\n",
            "\n",
            "Nurse:\n",
            "No more than you have of a man to be done:\n",
            "I am a servant is far in\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output = model.generate(\n",
        "    input_str,\n",
        "    max_gen_len = params.max_seq_len - len(input_str),\n",
        "    memory_saver_div = 8,\n",
        "    temperature = 0.6,\n",
        "    top_p = 0.9,\n",
        "    top_k = 32,\n",
        ")\n",
        "print(output)"
      ],
      "metadata": {
        "id": "Hikwp10DQQEb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a3649ce-285b-4bac-f764-bf585c0db5b4"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "maximum attention matrix size will be 64x512 rather than 512x512\n",
            "\n",
            "JULIET:\n",
            "O Romeo, Romeo! wherefore art thou Romeo?\n",
            "\n",
            "Nurse:\n",
            "I will be satisfied, for I am so speak too.\n",
            "\n",
            "JULIET:\n",
            "I will confess to me with him to the king.\n",
            "\n",
            "Nurse:\n",
            "I am so farther to my heart as we can.\n",
            "\n",
            "Nurse:\n",
            "\n",
            "Nurse:\n",
            "No, if it be so, sir.\n",
            "\n",
            "Nurse:\n",
            "Good friend, I will. Friar, ho!\n",
            "\n",
            "Nurse:\n",
            "I am the devil of this man in a fear,\n",
            "That it will not what you shall see him hate,\n",
            "That I will have done to be a man's death.\n",
            "\n",
            "JULIET:\n",
            "Go, then, what news?\n",
            "\n",
            "Nurse:\n",
            "I have been gone.\n",
            "\n",
            "Nurse:\n",
            "What says the care?\n",
            "\n",
            "JULIET:\n",
            "Ay, for I will content it, I would not know.\n",
            "\n",
            "JULIET:\n",
            "I know my son, my lord.\n",
            "\n",
            "JULIET:\n",
            "I do not go to be home.\n",
            "\n",
            "JULIET:\n",
            "I am a little day of my bosom.\n",
            "\n",
            "Nurse:\n",
            "Then then, and says the king hath been my face?\n",
            "\n",
            "Nurse:\n",
            "I am so hour to me; and my fault is in mine.\n",
            "\n",
            "Nurse:\n",
            "I will be her hence shall I be so draw there.\n",
            "\n",
            "Nurse:\n",
            "That is my\n"
          ]
        }
      ]
    }
  ]
}